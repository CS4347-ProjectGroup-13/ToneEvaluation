{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "from dataset import FusedSentenceMichigan\n",
    "USE_REAL_DATA = False\n",
    "DATASET =FusedSentenceMichigan(feature_options={},dataset_size=500, overlaps=[1000],use_real_data=USE_REAL_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataset import read_aidatatang_index,read_aidatatang_data\n",
    "\n",
    "# AIDTANG = read_aidatatang_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "from dataset import single_segmentation\n",
    "import torchaudio\n",
    "import torchaudio\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"./resources/nihao.wav\"\n",
    "\n",
    "# Load the audio file\n",
    "waveform, sample_rate = torchaudio.load(file_path)\n",
    "waveform =  torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "waveform = waveform[0].unsqueeze(0)\n",
    "text = [\"ä½  hao jing tian mei you chi fan\"]\n",
    "\n",
    "print(sample_rate)\n",
    "Audio(waveform.numpy(), rate=sample_rate)\n",
    "single_segmentation(waveform,originallen=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "from dataset import run_segmentation\n",
    "\n",
    "SEGMENTATION_RESULTS, DATASET_PROPERTIES = run_segmentation(dataset=DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PROPERTIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "from dataset import processes_segementation_results_global\n",
    "PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET = processes_segementation_results_global(SEGMENTATION_RESULTS,real=USE_REAL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_SEGEMENTATION_DATASET[\"diff\"].value_counts(normalize=True)\n",
    "PROCESSED_SEGEMENTATION_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_segmentations_to_index(segmentations, convert_fn = lambda x: x.replace(\"5\", \"4\"), real = False):\n",
    "    finalResults = []\n",
    "    for i in range(len(segmentations)):\n",
    "        results_entry = segmentations.iloc[i]\n",
    "        word_count = results_entry['word_count']\n",
    "        sentence = results_entry['sentence_results']\n",
    "        mappings = results_entry['mappings']\n",
    "\n",
    "        if real and results_entry['diff'] != 0:\n",
    "            continue\n",
    "\n",
    "        split_sentence = sentence.split(\"_\")\n",
    "        split_sentence = [convert_fn(x) for x in split_sentence]\n",
    "        for j in range(len(split_sentence)):\n",
    "            word_id = split_sentence[j] \n",
    "            toneclass = int(word_id[-1])\n",
    "            main_Idx = i\n",
    "            word_Idx = j\n",
    "            mapped_onset = mappings[j]\n",
    "            finalResults.append((word_id, toneclass, main_Idx, word_Idx,mapped_onset))\n",
    "        \n",
    "    return pd.DataFrame(finalResults, columns=['word_id', 'toneclass', 'main_Idx', 'word_Idx', 'mapped_onset'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PSEG_INDEX = convert_segmentations_to_index(PROCESSED_SEGEMENTATION_DATASET, real=USE_REAL_DATA)\n",
    "PSEG_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def get_audio_sample_at_idx(idx,pSEG_index, pSEG, pSEGAUDIO, sr= 16000, max_time=1.5):\n",
    "    sample_info = pSEG_index.iloc[idx]\n",
    "    pSEG_idx = sample_info['main_Idx']\n",
    "\n",
    "    sentence_info = pSEG.iloc[pSEG_idx]\n",
    "    pSEG_audio = pSEGAUDIO.iloc[pSEG_idx][\"audio\"]\n",
    "\n",
    "\n",
    "    segementation_start_time_gt = sentence_info['delimiter_time_results']\n",
    "    if len(segementation_start_time_gt) == sentence_info['word_count']:\n",
    "        segementation_start_time_gt = segementation_start_time_gt + [segementation_start_time_gt[-1] + max_time]\n",
    "\n",
    "\n",
    "    segementation_start_time = sorted(sentence_info['segementation_times_results'] + [segementation_start_time_gt[sentence_info['word_count']]])\n",
    "    segmentation_onset_mapping = sentence_info['mappings']\n",
    "    mapped_onset = sample_info['mapped_onset']\n",
    "\n",
    "    current_mapping = segmentation_onset_mapping[sample_info['word_Idx']]\n",
    "\n",
    "    # print(sample_info)\n",
    "    # print(sentence_info)\n",
    "    # print(segementation_start_time_gt)\n",
    "    if current_mapping == -1:\n",
    "        # unmapped, use previous result\n",
    "        prev_mapping = 0\n",
    "        for i in range(sample_info['word_Idx'], -1, -1):\n",
    "            if segmentation_onset_mapping[i] != -1:\n",
    "                prev_mapping = segmentation_onset_mapping[i]\n",
    "                break\n",
    "        # prev_mapping = segmentation_onset_mapping[sample_info['word_Idx'] - 1]\n",
    "        start_time = segementation_start_time[prev_mapping]\n",
    "        end_time = segementation_start_time[prev_mapping+1] \n",
    "        # print(idx,prev_mapping,prev_mapping+1)\n",
    "    else:\n",
    "        start_time = segementation_start_time[current_mapping] \n",
    "        end_time = segementation_start_time[current_mapping+1]\n",
    "\n",
    "    start_samples = librosa.time_to_samples(start_time, sr=sr)\n",
    "    if end_time-start_time > max_time:\n",
    "        end_time = start_time + max_time\n",
    "    end_samples = librosa.time_to_samples(end_time, sr=sr)\n",
    "\n",
    "    return sample_info,pSEG_audio[start_samples:end_samples]\n",
    "print(get_audio_sample_at_idx(1121, PSEG_INDEX, PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET))\n",
    "# print(get_audio_sample_at_idx(720, PSEG_INDEX, PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dataset)\n",
    "from dataset import DatasetMichigan\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "INFO,AUDIO_SAMPLE = get_audio_sample_at_idx(56, PSEG_INDEX, PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET)\n",
    "MEL = DatasetMichigan.preproccessData(AUDIO_SAMPLE, features={\"mel_spectrogram\"},pad_audio=True, pad_samples = librosa.time_to_samples(1, sr=16000), sampling_rate = 16000)[1]\n",
    "\n",
    "# for i in range(len(PSEG_INDEX)):\n",
    "#     print(i)\n",
    "#     get_audio_sample_at_idx(i, PSEG_INDEX, PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET)\n",
    "\n",
    "\n",
    "# Plot the mel spectrogram\n",
    "print(INFO)\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(MEL, sr=16000, hop_length=321, x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.show()\n",
    "\n",
    "Audio(AUDIO_SAMPLE, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import *\n",
    "from hparams import *\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  ConfusionMatrixDisplay\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.buffer = {}\n",
    "        self.results_buffer = []\n",
    "\n",
    "    def update(self, out, tgt, loss):\n",
    "        with torch.no_grad():\n",
    "            out = out.argmax(dim=1)\n",
    "            out = torch.flatten(out)\n",
    "            tgt = torch.flatten(tgt)\n",
    "\n",
    "            acc = accuracy_score(tgt.cpu(), out.cpu())\n",
    "            f1 = f1_score(tgt.cpu(), out.cpu(), average='macro')\n",
    "\n",
    "            batch_metric = {\n",
    "                'loss': loss.item(),\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "            for k in batch_metric:\n",
    "                if k in self.buffer:\n",
    "                    self.buffer[k].append(batch_metric[k])\n",
    "                else:\n",
    "                    self.buffer[k] = [batch_metric[k]]\n",
    "\n",
    "            self.results_buffer.append((out.cpu(), tgt.cpu()))\n",
    "\n",
    "    def get_value(self):\n",
    "        for k in self.buffer:\n",
    "            self.buffer[k] = sum(self.buffer[k]) / len(self.buffer[k])\n",
    "        ret = self.buffer\n",
    "        ret2 = self.results_buffer\n",
    "        self.buffer = {}\n",
    "        self.results_buffer = []\n",
    "\n",
    "        gt= torch.cat([x[1] for x in ret2]).tolist()\n",
    "        pred= torch.cat([x[0] for x in ret2]).tolist()\n",
    "        report= classification_report(gt, pred, output_dict=False, digits=4)\n",
    "\n",
    "        return ret, report,gt,pred\n",
    "\n",
    "\n",
    "def validateModelonPregeneratedSegments(model, device):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    metric = Metrics()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(range(len(PSEG_INDEX)), desc='Validation', unit='sample')\n",
    "        final_predictions = []\n",
    "\n",
    "        for sample_idx, _ in enumerate(pbar):\n",
    "            info, sample = get_audio_sample_at_idx(sample_idx, PSEG_INDEX, PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET)\n",
    "\n",
    "            # mel_spectrogram_normalised_log_scale_torch, yin_normalised_torch, pyin_normalised_torch, word, tone_class = batch\n",
    "            mel = DatasetMichigan.preproccessData(sample, features={\"mel_spectrogram\"},pad_audio=True, pad_samples = librosa.time_to_samples(1.5, sr=16000), sampling_rate = 16000)[1]\n",
    "            mel_spectrogram_normalised_log_scale_torch = torch.from_numpy(mel).float()\n",
    "\n",
    "            tone_class = info['toneclass']\n",
    "            tone_class -= 1 # 0-index\n",
    "            tone_class = torch.tensor(tone_class)\n",
    "            tone_class = tone_class.unsqueeze(0)\n",
    "\n",
    "            x = mel_spectrogram_normalised_log_scale_torch.to(device)\n",
    "            x = x[None, None, :, :]\n",
    "            \n",
    "            tgt = tone_class.to(device)\n",
    "            out = model(x)\n",
    "            loss = loss_func(out, tgt)\n",
    "            metric.update(out, tgt, loss)\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "    metric_test, report,gt,pred = metric.get_value()\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(gt, pred, display_labels=[\"1\",\"2\",\"3\",\"4\"]).plot()\n",
    "    return metric_test, report\n",
    "\n",
    "\n",
    "model = ToneEval_Base(input_shape=(1, 128, 75))\n",
    "# fit(model, args=Hparams.args, learning_params=learning_params)\n",
    "model.load_state_dict(torch.load('results/config1_testMV1/best_model.pth'))\n",
    "validateModelonPregeneratedSegments(model, \"cuda\" )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
