{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "from dataset import FusedSentenceMichigan\n",
    "USE_REAL_DATA = False\n",
    "DATASET =FusedSentenceMichigan(feature_options={},dataset_size=500, overlaps=[1000],use_real_data=USE_REAL_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_segmentation_model\n",
    "a,b = load_segmentation_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(dataset)\n",
    "# from dataset import single_segmentation\n",
    "# import torchaudio\n",
    "# import torchaudio\n",
    "\n",
    "# # Specify the file path\n",
    "# file_path = \"./resources/audio.wav\"\n",
    "\n",
    "# # Load the audio file\n",
    "# waveform, sample_rate = torchaudio.load(file_path)\n",
    "# waveform =  torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "# waveform = waveform[0].unsqueeze(0)\n",
    "# text = [\"ä½  hao jing tian mei you chi fan\"]\n",
    "\n",
    "# print(sample_rate)\n",
    "# Audio(waveform.numpy(), rate=sample_rate)\n",
    "# print(waveform.shape)\n",
    "# single_segmentation(waveform,originallen=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "from dataset import run_segmentation\n",
    "\n",
    "SEGMENTATION_RESULTS, DATASET_PROPERTIES = run_segmentation(dataset=DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PROPERTIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "from dataset import processes_segementation_results_global\n",
    "PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET = processes_segementation_results_global(SEGMENTATION_RESULTS,real=USE_REAL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_SEGEMENTATION_DATASET[\"diff\"].value_counts(normalize=True)\n",
    "PROCESSED_SEGEMENTATION_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset import convert_segmentations_to_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PSEG_INDEX = convert_segmentations_to_index(PROCESSED_SEGEMENTATION_DATASET, real=USE_REAL_DATA)\n",
    "PSEG_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from dataset import get_audio_sample_at_idx\n",
    "\n",
    "# print(get_audio_sample_at_idx(720, PSEG_INDEX, PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dataset)\n",
    "from dataset import DatasetMichigan\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "INFO,AUDIO_SAMPLE = get_audio_sample_at_idx(56, PSEG_INDEX, PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET)\n",
    "MEL = DatasetMichigan.preproccessData(AUDIO_SAMPLE, features={\"mel_spectrogram\"},pad_audio=True, pad_samples = librosa.time_to_samples(1, sr=16000), sampling_rate = 16000)[1]\n",
    "\n",
    "# for i in range(len(PSEG_INDEX)):\n",
    "#     print(i)\n",
    "#     get_audio_sample_at_idx(i, PSEG_INDEX, PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET)\n",
    "\n",
    "\n",
    "# Plot the mel spectrogram\n",
    "print(INFO)\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(MEL, sr=16000, hop_length=321, x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.show()\n",
    "\n",
    "Audio(AUDIO_SAMPLE, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import *\n",
    "from hparams import *\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  ConfusionMatrixDisplay\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.buffer = {}\n",
    "        self.results_buffer = []\n",
    "\n",
    "    def update(self, out, tgt, loss):\n",
    "        with torch.no_grad():\n",
    "            out = out.argmax(dim=1)\n",
    "            out = torch.flatten(out)\n",
    "            tgt = torch.flatten(tgt)\n",
    "\n",
    "            acc = accuracy_score(tgt.cpu(), out.cpu())\n",
    "            f1 = f1_score(tgt.cpu(), out.cpu(), average='macro')\n",
    "\n",
    "            batch_metric = {\n",
    "                'loss': loss.item(),\n",
    "                'accuracy': acc,\n",
    "                'f1': f1,\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "            for k in batch_metric:\n",
    "                if k in self.buffer:\n",
    "                    self.buffer[k].append(batch_metric[k])\n",
    "                else:\n",
    "                    self.buffer[k] = [batch_metric[k]]\n",
    "\n",
    "            self.results_buffer.append((out.cpu(), tgt.cpu()))\n",
    "\n",
    "    def get_value(self):\n",
    "        for k in self.buffer:\n",
    "            self.buffer[k] = sum(self.buffer[k]) / len(self.buffer[k])\n",
    "        ret = self.buffer\n",
    "        ret2 = self.results_buffer\n",
    "        self.buffer = {}\n",
    "        self.results_buffer = []\n",
    "\n",
    "        gt= torch.cat([x[1] for x in ret2]).tolist()\n",
    "        pred= torch.cat([x[0] for x in ret2]).tolist()\n",
    "        report= classification_report(gt, pred, output_dict=False, digits=4)\n",
    "\n",
    "        return ret, report,gt,pred\n",
    "\n",
    "\n",
    "def validateModelonPregeneratedSegments(model, device, data):\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    metric = Metrics()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(range(len(PSEG_INDEX)), desc='Validation', unit='sample')\n",
    "        final_predictions = []\n",
    "\n",
    "        for sample_idx, _ in enumerate(pbar):\n",
    "            info, sample = get_audio_sample_at_idx(sample_idx, PSEG_INDEX, PROCESSED_SEGEMENTATION_DATASET, PROCESSED_SEGMENTAION_AUDIO_DATASET)\n",
    "\n",
    "            # mel_spectrogram_normalised_log_scale_torch, yin_normalised_torch, pyin_normalised_torch, word, tone_class = batch\n",
    "            mel = DatasetMichigan.preproccessData(sample, features={\"mel_spectrogram\"},pad_audio=True, pad_samples = librosa.time_to_samples(1.5, sr=16000), sampling_rate = 16000)[1]\n",
    "            mel_spectrogram_normalised_log_scale_torch = torch.from_numpy(mel).float()\n",
    "\n",
    "            tone_class = info['toneclass']\n",
    "            tone_class -= 1 # 0-index\n",
    "            tone_class = torch.tensor(tone_class)\n",
    "            tone_class = tone_class.unsqueeze(0)\n",
    "\n",
    "            x = mel_spectrogram_normalised_log_scale_torch.to(device)\n",
    "            x = x[None, None, :, :]\n",
    "            \n",
    "            tgt = tone_class.to(device)\n",
    "            out = model(x)\n",
    "            loss = loss_func(out, tgt)\n",
    "            metric.update(out, tgt, loss)\n",
    "            pbar.set_postfix({'Loss': loss.item()})\n",
    "    metric_test, report,gt,pred = metric.get_value()\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(gt, pred, display_labels=[\"1\",\"2\",\"3\",\"4\"]).plot()\n",
    "    return metric_test, report,gt,pred\n",
    "\n",
    "\n",
    "model = ToneEval_Base(input_shape=(1, 128, 75))\n",
    "# fit(model, args=Hparams.args, learning_params=learning_params)\n",
    "model.load_state_dict(torch.load('results/config1_testMV1/best_model.pth'))\n",
    "validateModelonPregeneratedSegments(model, \"cuda\" )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
