{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device, MyDataset\n",
    "from hparams import Hparams\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "args = Hparams.args\n",
    "dataset = MyDataset(\n",
    "        dataset_root=args['dataset_root'],\n",
    "        split='train',\n",
    "        sampling_rate=args['sampling_rate'],\n",
    "        sample_length=args['sample_length'],\n",
    "        frame_size=args['frame_size'],\n",
    "        song_fns=None,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import  read_michigan_dataset_index\n",
    "read_michigan_dataset_index()\n",
    "\n",
    "class PhomemeLibrary():\n",
    "    def __init__(self, audio_source = (\"michigan\", \"MV1\")) -> None:\n",
    "\n",
    "        # handling source audio\n",
    "        self.audio_source = audio_source\n",
    "        self.index = None\n",
    "        if audio_source[0] == \"michigan\":\n",
    "            ds_idx = read_michigan_dataset_index()\n",
    "            self.index = ds_idx[ds_idx[\"participantID\"] == audio_source[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def testItem(idx=0):\n",
    "    print(f\"samplerate: {dataset.sampling_rate }\")\n",
    "    pYinFrameTime = librosa.frames_to_time(dataset.sample_length, sr=dataset.sampling_rate, hop_length=200)\n",
    "    melspecFrameTime = librosa.frames_to_time(dataset.sample_length, sr=dataset.sampling_rate, hop_length=321)\n",
    "    print(f\"melspec FrameTime: {melspecFrameTime}\")\n",
    "    print(f\"pYin FrameTime: {pYinFrameTime}\")\n",
    "    \n",
    "    mel_spectrogram, yin, pyin = dataset.__getitem__(idx)\n",
    "    print(mel_spectrogram.shape[1])\n",
    "    print(yin.shape)\n",
    "    print(pyin.shape)\n",
    "\n",
    "testItem(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import pinyin\n",
    "\n",
    "\n",
    "def read_aidatatang_index(data_root=os.path.join(os.getcwd(),\"data_full\")):\n",
    "\n",
    "    # handling transcripts\n",
    "    transcript_path = os.path.join(data_root, 'aidatatang', 'transcript', 'aidatatang_200_zh_transcript.txt')\n",
    "    with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    def parseLine(line):\n",
    "        split = line.split(' ')\n",
    "        transcriptIDX = split[0]\n",
    "        # T0055 G0002 S0002\n",
    "        prefix = transcriptIDX[:5]\n",
    "        participant_id = transcriptIDX[5:10]\n",
    "        sentence_id = transcriptIDX[10:]\n",
    "        \n",
    "        text = split[1:]\n",
    "        text,text_ids = extract_pinyin(list(' '.join(text).strip()))\n",
    "        return participant_id,sentence_id, text,text_ids\n",
    "    \n",
    "    lines=[parseLine(line) for line in lines]\n",
    "\n",
    "    # handling actual index\n",
    "    def read_subfolder(subfolder_path):\n",
    "        files = os.listdir(subfolder_path)\n",
    "        return files\n",
    "    \n",
    "    subfolders = [\"dev\",\"test\",\"train\"]\n",
    "    subfolder_index= {subfolder:sorted(read_subfolder(os.path.join(data_root, 'aidatatang', 'corpus', subfolder))) for subfolder in subfolders}\n",
    "    for subfolder in subfolders:\n",
    "        indexing = set(subfolder_index[subfolder])\n",
    "        # sanity\n",
    "        assert len(indexing) == len(subfolder_index[subfolder])\n",
    "        subfolder_index[subfolder] = indexing \n",
    "\n",
    "    df = pd.DataFrame.from_records(data = lines, columns=[\"participantID\", \"sentenceID\", \"transcript\", \"toneclass\"])\n",
    "    \n",
    "    def get_category(x):\n",
    "        pid = x[\"participantID\"]\n",
    "        fname = f\"{pid}.tar.gz\"\n",
    "        for subfolder in subfolders:\n",
    "            if fname in subfolder_index[subfolder]:\n",
    "                return subfolder\n",
    "        raise ValueError(f\"Could not find {fname} in any subfolder\")\n",
    "\n",
    "    df[\"folder\"] = df.apply(get_category, axis=1)\n",
    "    return df\n",
    "\n",
    "def extract_pinyin(sentence_word_list):\n",
    "    pinyin_word_list = tuple([pinyin.get(x, format=\"numerical\", delimiter=\" \") for x in sentence_word_list])\n",
    "    pinyin_word_list_tone_class = tuple([int(x[-1]) if len(x)>1 else 0 for x in pinyin_word_list])\n",
    "    return pinyin_word_list,pinyin_word_list_tone_class\n",
    "    \n",
    "def read_aidatatang_data(participantID, sentenceID):\n",
    "\n",
    "    fullFileName = f\"T0055{participantID}{sentenceID}\"\n",
    "    data_root=os.path.join(os.getcwd(),\"data_full\")\n",
    "    makePath = lambda x,y: os.path.join(data_root, 'aidatatang', 'corpus', x, f\"{y}.tar.gz\")\n",
    "    subfolders = [\"dev\",\"test\",\"train\"]\n",
    "    possiblePaths = [makePath(subfolder,participantID) for subfolder in subfolders]\n",
    "\n",
    "    zipped_path = [path for path in possiblePaths if os.path.exists(path)][0]\n",
    "\n",
    "    \n",
    "    suffixed = {\n",
    "        'AudioData': (\".wav\",lambda x: librosa.load(x, sr = 16000, mono=True)[0]) ,\n",
    "        \"MetaData\": (\".txt\", lambda x: tuple(extract_pinyin(x.read().decode(\"utf-8\")))), \n",
    "        \"Transcript\": (\".trn\", lambda x: x.read().decode(\"utf-8\"))\n",
    "        }\n",
    "\n",
    "    with tarfile.open(zipped_path, 'r',) as tar_ref:\n",
    "\n",
    "        data = {}\n",
    "        for suf in suffixed.items():\n",
    "            file_to_extract = f\"./{participantID}/{fullFileName}{suf[1][0]}\"\n",
    "            print(file_to_extract)\n",
    "            tar_ref.extractfile(file_to_extract)\n",
    "            data[suf[0]] = suf[1][1](tar_ref.extractfile(file_to_extract))\n",
    "        \n",
    "    for x in data:\n",
    "        print(x, data[x])\n",
    "\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "kiki = read_aidatatang_index()\n",
    "# d = read_aidatatang_data(*read_aidatatang_index()[0][3][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiki[kiki[\"folder\"] == 'dev'].drop_duplicates(subset = \"transcript\", keep= \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_aidatatang_data(\"G0002\", \"S0001\")\n",
    "\n",
    "librosa.display.waveshow(d['AudioData'], sr=16000)\n",
    "Audio(d['AudioData'], rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fundamentals(size, lower, upper, is_mel=False):\n",
    "    if is_mel:\n",
    "        freqs = librosa.mel_frequencies(n_mels=size)\n",
    "    else:\n",
    "        freqs = librosa.fft_frequencies(sr=16000, n_fft=size)\n",
    "    \n",
    "    mask  = (freqs<=upper) & ( freqs>=lower) & ( freqs!=0)\n",
    "    fundamental_idxs = {x:y for y,x in enumerate(freqs.tolist())}\n",
    "    fundamentals = freqs[mask]\n",
    "\n",
    "    fmax = np.max(freqs)\n",
    "    \n",
    "    fundamentals_dict = {f:([],[]) for f in fundamentals}\n",
    "\n",
    "    print(fmax)\n",
    "    print(freqs)\n",
    "    \n",
    "    for k,v in fundamentals_dict.items():\n",
    "        f = k\n",
    "        harmonoic = 1\n",
    "        while f <= fmax:\n",
    "            f = f*harmonoic\n",
    "            if f > fmax:\n",
    "                break\n",
    "            v[0].append(f)\n",
    "            v[1].append(fundamental_idxs[f])\n",
    "            harmonoic += 1\n",
    "\n",
    "    return fundamentals_dict\n",
    "\n",
    "def spectral_folding(folddict, spectrum):\n",
    "    folded = []\n",
    "\n",
    "    for k in sorted(folddict.keys()):\n",
    "        idxs = folddict[k][1]\n",
    "        folded[k] = folddict[v[1]]\n",
    "    return folded\n",
    "\n",
    "calculate_fundamentals(1024, 0, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the base frequency and number of chroma bins\n",
    "base_freq = 100\n",
    "n_chroma = 50\n",
    "\n",
    "\n",
    "# Calculate the chroma filter\n",
    "chroma_filter = librosa.filters.chroma(sr=16000, n_fft=4096, n_chroma=n_chroma, tuning=base_freq, norm=0,ctroct=4,octwidth=2)\n",
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(chroma_filter, x_axis='linear', ax=ax)\n",
    "print(chroma_filter.shape)\n",
    "ax.set(ylabel='Chroma filter', title='Chroma filter bank')\n",
    "fig.colorbar(img, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectralFlux(a,b):\n",
    "    d = read_aidatatang_data(a,b)\n",
    "    audioData = d['AudioData']\n",
    "    # sosfilt = signal.butter(2,(20,200), 'bp', fs=16000, output='sos')\n",
    "    # audioData = signal.sosfilt(sosfilt, audioData)\n",
    "    # audioData = audioData[np.abs(audioData)<np.mean(np.abs(audioData))]\n",
    "\n",
    "    melspec = librosa.feature.melspectrogram(y= audioData, sr=16000, n_fft=2048, hop_length=128, n_mels=256)\n",
    "    print(melspec.shape)\n",
    "    # chroma = librosa.feature.chroma_stft(y=audioData, sr=16000, n_fft=2048, hop_length=128, n_chroma=50, tuning=100, \n",
    "    # norm=0,ctroct=4,octwidth=2)\n",
    "    base_freq =100\n",
    "    n_chroma = 50\n",
    "    hop_length = 8\n",
    "    chroma_filter = librosa.filters.chroma(sr=16000, n_fft=1024, n_chroma=n_chroma, tuning=base_freq, norm=2,ctroct=4,octwidth=5,base_c=True)\n",
    "    mag_spectrum = np.abs(librosa.stft(audioData, n_fft=1024, hop_length=hop_length))\n",
    "    chroma_spectrum = np.dot(chroma_filter, mag_spectrum)\n",
    "\n",
    "    # use the yin algo to get yin too\n",
    "    yin = librosa.yin(audioData, sr=16000, hop_length=hop_length, fmin=100, fmax = 200)\n",
    "\n",
    "    # for each window, find the min that is significant\n",
    "    # find the highest power bin\n",
    "    mag_spectrum_power_max = np.argmax(mag_spectrum, axis=0)\n",
    "\n",
    "    mag_spectrum_power = np.zeros_like(mag_spectrum)\n",
    "    for i in range(mag_spectrum.shape[1]):\n",
    "        mag_spectrum_power[mag_spectrum_power_max[i],i] = 1\n",
    "\n",
    "    print(mag_spectrum.shape)\n",
    "    print(mag_spectrum_power)\n",
    "\n",
    "    # try a wavelet transform using libros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_michigan_dataset_index(data_root=os.path.join(os.getcwd(),\"data_full\")):\n",
    "\n",
    "    # handling transcripts\n",
    "    audio = os.path.join(data_root, 'michigan', 'tone_perfect_all_mp3', 'tone_perfect')\n",
    "    transcripts= os.path.join(data_root, 'michigan', 'tone_perfect_all_xml', 'tone_perfect')\n",
    "    \n",
    "    audioIndex = os.listdir(audio)\n",
    "    transcriptIndex = os.listdir(transcripts)\n",
    "    # ignoreing the metadata for now\n",
    "    \n",
    "    def parseAudioIndex(filename):\n",
    "        elem = filename.split(\"_\")\n",
    "        word = elem[0]\n",
    "        word_tone_class = int(word[-1]) \n",
    "        particpantID = elem[1]\n",
    "        xml_fn = filename.replace(\".mp3\", \".xml\").replace(\"MP3\", \"CUSTOM\")\n",
    "        return (particpantID, word, word_tone_class, filename,xml_fn)\n",
    "    \n",
    "    audioData = [parseAudioIndex(filename) for filename in audioIndex]\n",
    "    return pd.DataFrame.from_records(data=audioData, columns=[\"participantID\", \"word\", \"toneclass\", \"filename\",'xml_fn'])\n",
    "\n",
    "def read_michigan_dataset_audio(filename, \n",
    "                                data_root=os.path.join(os.getcwd(),\"data_full\"),\n",
    "                                sr = 16000,\n",
    "                                mono=True\n",
    "                                ):\n",
    "    filepath = os.path.join(data_root, 'michigan', 'tone_perfect_all_mp3', 'tone_perfect', filename)\n",
    "    return librosa.load(filepath, sr=sr, mono=mono)[0]\n",
    "\n",
    "md = read_michigan_dataset_index()\n",
    "\n",
    "md_dict = set(md[\"word\"].unique())\n",
    "# sanity\n",
    "samplesentence = ['yi3', 'hou4', 'ni3', 'shi4', 'nan2', 'hai2', 'zi3']\n",
    "assert set(samplesentence).issubset(md_dict)\n",
    "\n",
    "def getSentence(pid,words, md=md, convert_fn = lambda x: x.replace(\"5\", \"4\")):\n",
    "    playerdata = md[md[\"participantID\"]==pid]\n",
    "\n",
    "    filenames  = []\n",
    "    for word in words:\n",
    "        word = convert_fn(word)\n",
    "\n",
    "        words = playerdata[playerdata[\"word\"]==word]\n",
    "        if len(words) == 0:\n",
    "            raise ValueError(f\"Could not find {word} in any subfolder\")\n",
    "        filenames.append(words.iloc[0][\"filename\"])\n",
    "\n",
    "    audiosamples = [read_michigan_dataset_audio(filename) for filename in filenames]\n",
    "    return audiosamples\n",
    "md\n",
    "audiosamples = getSentence(\"MV1\", samplesentence)\n",
    "\n",
    "\n",
    "def mix_audio(audiosamples, overlap = 0, add_silence = 1, signal_length_seconds = None , min_samples_each_word = 0):\n",
    "    frames_to_add = librosa.time_to_samples(add_silence, sr=16000)\n",
    "    lens = [len(x) for x in audiosamples]\n",
    "    total_len = 0\n",
    "\n",
    "    if overlap == \"auto\" and not (signal_length_seconds is None):\n",
    "        signal_samples = librosa.time_to_samples(signal_length_seconds, sr=16000)\n",
    "        actual_total_len = np.sum(lens)\n",
    "        overlap = (actual_total_len - signal_samples)/(len(lens)-1)\n",
    "        assert overlap > 0\n",
    "        for i in lens:\n",
    "            if overlap > i:\n",
    "                raise ValueError(f\"Overlap {overlap} is larger than audio sample {i}\")\n",
    "        \n",
    "    for idx,l in enumerate(lens):\n",
    "        if idx == 0:\n",
    "            total_len += l\n",
    "        else:\n",
    "            total_len += l - overlap\n",
    "            \n",
    "    final = np.zeros(total_len+frames_to_add+frames_to_add)\n",
    "\n",
    "    base_frame_index = frames_to_add\n",
    "    current_id = base_frame_index\n",
    "    delims = []\n",
    "    delims.append(current_id)\n",
    "    for idx,a in enumerate(audiosamples):\n",
    "        audLen = len(a)\n",
    "        if idx == 0:\n",
    "            final[current_id:current_id+audLen] = a\n",
    "            current_id = current_id+audLen\n",
    "        else:\n",
    "            current_id -= overlap\n",
    "            if current_id - delims[-1] < min_samples_each_word:\n",
    "                current_id = delims[-1] + min_samples_each_word\n",
    "            delims.append(current_id)\n",
    "            final[current_id:current_id+audLen] = a\n",
    "            current_id = current_id+audLen\n",
    "    delims.append(current_id)\n",
    "    return final, delims\n",
    "\n",
    "\n",
    "mixed,_ = mix_audio(audiosamples, overlap=1000)\n",
    "librosa.display.waveshow(mixed, sr=16000)\n",
    "plt.show()\n",
    "\n",
    "Audio(data=mixed, rate=16000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from librosa.sequence import dtw\n",
    "import librosa\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def dtw_distance(x, y, sr=16000, hop_length=128):\n",
    "    \"\"\"\n",
    "    Computes the dynamic time warping distance between two audio signals x and y.\n",
    "    \"\"\"\n",
    "    x_harmonic, x_percussive = librosa.effects.hpss(x)\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "\n",
    "    x = x_percussive\n",
    "    y = y_percussive\n",
    "    \n",
    "\n",
    "    x_mfcc = librosa.feature.melspectrogram(y= x, sr=16000, n_fft=512, hop_length=hop_length, n_mels=128)\n",
    "    y_mfcc = librosa.feature.melspectrogram(y= y, sr=16000, n_fft=512, hop_length=hop_length, n_mels=128)\n",
    "\n",
    "    # convert to power specs\n",
    "\n",
    "    # set the bottle halves of both to 0\n",
    "    x_mfcc[:x_mfcc.shape[0]//3,:] = 0\n",
    "    y_mfcc[:y_mfcc.shape[0]//3,:] = 0\n",
    "\n",
    "    # # set the top havles of both to 0\n",
    "    # x_mfcc[x_mfcc.shape[0]//2:,:] = 0\n",
    "    # y_mfcc[y_mfcc.shape[0]//2:,:] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    D, wp = dtw(x_mfcc, y_mfcc)\n",
    "    return D, wp , x_mfcc, y_mfcc\n",
    "\n",
    "# please plot the wraping path\n",
    "def plot_warping_path(wp, D):\n",
    "    \"\"\"\n",
    "    Plots the warping path on the distance matrix.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(D, origin='lower', cmap='gray', interpolation='nearest')\n",
    "    plt.plot(wp[:, 1], wp[:, 0], marker='o', color='r')\n",
    "    plt.xlim([-0.5, D.shape[1]-0.5])\n",
    "    plt.ylim([-0.5, D.shape[0]-0.5])\n",
    "    plt.title('Warping path on distance matrix $D$')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_warping(mfcc1, mfcc2, wp, delims_a, delims_b):\n",
    "    \"\"\"\n",
    "    Plots the two MFCCs with lines indicating the warping path.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n",
    "    ax = [ax]\n",
    "    # make sure both are in db\n",
    "    spec1 = librosa.amplitude_to_db(mfcc1)\n",
    "    spec2 = librosa.amplitude_to_db(mfcc2)\n",
    "\n",
    "    # stack spec1  and spec2 them tgt\n",
    "    spec = np.hstack([spec1, spec2])\n",
    "\n",
    "    librosa.display.specshow(spec, sr=16000, x_axis='time', ax=ax[0])\n",
    "    \n",
    "\n",
    "    # wp_time = librosa.frames_to_time(wp[:, 1])\n",
    "    # for i in range(wp.shape[0]):\n",
    "    #     ax[0].plot([wp_time[i], wp_time[i]], [0, mfcc1.shape[0]-1], 'r')\n",
    "    #     ax[1].plot([wp_time[i], wp_time[i]], [0, mfcc2.shape[0]-1], 'r')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def calculate_delimiter_timing_mapping(delimiters_a, wp, sr=16000, hop_length=128):\n",
    "    \"\"\"\n",
    "    Calculates the timing of the delimiters in seconds.\n",
    "    \"\"\"\n",
    "    frame_time = librosa.frames_to_time(1, sr=sr, hop_length=hop_length)\n",
    "    delimiters_a_frames = librosa.samples_to_frames(delimiters_a, hop_length=hop_length)    \n",
    "    delimiters_b_frames = set()\n",
    "\n",
    "    a_done = set()\n",
    "\n",
    "    for i in range(wp.shape[0]):\n",
    "        mapping = wp[i]\n",
    "        if (mapping[0] in delimiters_a_frames) and (mapping[0] not in a_done):\n",
    "            a_done.add(mapping[0])\n",
    "            delimiters_b_frames.add(mapping[1])\n",
    "\n",
    "    delimiters_a_time = librosa.frames_to_time(delimiters_a_frames, sr=sr, hop_length=hop_length)\n",
    "    delimiters_b_time = librosa.frames_to_time(sorted(list(delimiters_b_frames)), sr=sr,hop_length=hop_length)\n",
    "    return delimiters_a_time, delimiters_b_time\n",
    "\n",
    "\n",
    "def get_non_silent(audio_sample, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Returns the start and end indexes for non-silent periods in an audio sample.\n",
    "    \"\"\"\n",
    "    # Compute the short-term energy of the audio sample\n",
    "    window_size = 1024\n",
    "    hop_size = 512\n",
    "    energy = np.array([sum(abs(audio_sample[i:i+window_size]**2)) for i in range(0, len(audio_sample)-window_size, hop_size)])\n",
    "    \n",
    "    # Normalize the energy\n",
    "    energy /= max(energy)\n",
    "    \n",
    "    # Find the start and end indexes for non-silent periods\n",
    "    non_silent = np.where(energy > threshold)[0]\n",
    "    start = non_silent[0] * hop_size\n",
    "    end = (non_silent[-1] + 1) * hop_size\n",
    "    return start, end\n",
    "\n",
    "def plot_audio_with_lines(audio_signal, sr=16000, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Plots an audio signal with red lines indicating the start and end of non-silent periods.\n",
    "    \"\"\"\n",
    "    start, end = get_non_silent(audio_signal, threshold=threshold)\n",
    "    duration = librosa.get_duration(y = audio_signal, sr=sr)\n",
    "    time = np.linspace(0, duration, len(audio_signal))\n",
    "    plt.plot(time, audio_signal)\n",
    "    plt.axvline(x=start/sr, color='r')\n",
    "    plt.axvline(x=end/sr, color='r')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_melspecs_with_lines(melspec1, melspec2, frame_indexes1, frame_indexes2, labels=None, hop_length=128):\n",
    "    \"\"\"\n",
    "    Plots two mel spectrograms side by side with vertical lines at the specified frame indexes.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 4))\n",
    "    img1 = librosa.display.specshow(librosa.power_to_db(melspec1, ref=np.max), x_axis='time', y_axis='mel', sr=16000, ax=axs[0], hop_length=hop_length)\n",
    "    img2 = librosa.display.specshow(librosa.power_to_db(melspec2, ref=np.max), x_axis='time', y_axis='mel', sr=16000, ax=axs[1],hop_length=hop_length)\n",
    "    for i, frame_index in enumerate(frame_indexes1):\n",
    "        axs[0].axvline(x=frame_index, color='r')\n",
    "        if labels is not None:\n",
    "            axs[0].text(frame_index, axs[0].get_ylim()[1]+2, labels[i], ha='center', va='bottom', fontsize=6, color='r', rotation=90,backgroundcolor = 'w')\n",
    "    for i, frame_index in enumerate(frame_indexes2):\n",
    "        axs[1].axvline(x=frame_index, color='r')\n",
    "        if labels is not None:\n",
    "            axs[1].text(frame_index, axs[1].get_ylim()[1]+2, labels[i], ha='center', va='bottom', fontsize=6, color='r', rotation=90, backgroundcolor = 'w')\n",
    "    fig.colorbar(img1, ax=axs[0], format='%+2.0f dB')\n",
    "    fig.colorbar(img2, ax=axs[1], format='%+2.0f dB')\n",
    "    plt.margins(0, 0)\n",
    "    plt.show()\n",
    "\n",
    "def breakupAudio(incoming, delims_in_time, silence_duration=0.5, feathering = 0.1):\n",
    "    delimsamples = librosa.time_to_samples(delims_in_time, sr=16000)\n",
    "    silencesamples = librosa.time_to_samples(silence_duration, sr=16000)\n",
    "    feather_samples = librosa.time_to_samples(feathering, sr=16000)\n",
    "    \n",
    "    silence_arr = np.zeros(silencesamples)\n",
    "    brokenSamples = []\n",
    "\n",
    "    for i in range(len(delims_in_time)-1):\n",
    "        start = delimsamples[i] - feather_samples\n",
    "        end = delimsamples[i+1] + feather_samples\n",
    "        sample = incoming[start:end]\n",
    "        brokenSamples.append(sample)\n",
    "        brokenSamples.append(silence_arr)\n",
    "    return np.concatenate(brokenSamples)\n",
    "\n",
    "\n",
    "def direct_convert(dlim_time_a, dlim_time_b):\n",
    "    start_a = dlim_time_a[0]\n",
    "    end_a = dlim_time_a[-1]\n",
    "    start_b = dlim_time_b[0]\n",
    "    end_b = dlim_time_b[-1]\n",
    "\n",
    "    dur_a = end_a - start_a\n",
    "    dur_b = end_b - start_b\n",
    "\n",
    "    # conver all a timings to scale to b timings\n",
    "    a_percentages = [(x-start_a)/dur_a for x in dlim_time_a]\n",
    "    new_b = [start_b + x*dur_b for x in a_percentages]\n",
    "    return np.array(new_b)\n",
    "\n",
    "def try_dtw():\n",
    "    aidtang = read_aidatatang_data(\"G0002\", \"S0002\")\n",
    "    actual = aidtang[\"AudioData\"]\n",
    "    sentence_list = aidtang[\"MetaData\"][0]\n",
    "    print(sentence_list[-1])\n",
    "    if \"\\n\" in sentence_list[-1]:\n",
    "        sentence_list = sentence_list[:-1]\n",
    "    start, end = get_non_silent(actual, threshold=0.1)\n",
    "    sig_len = librosa.samples_to_time(end-start, sr=16000)\n",
    "\n",
    "    padding = 0.3\n",
    "    padding_samples = librosa.time_to_samples(padding, sr=16000)\n",
    "    actual = actual[start-padding_samples:end+padding_samples]\n",
    "\n",
    "\n",
    "    generated, delimiters = mix_audio(\n",
    "        getSentence(\"MV1\", sentence_list),\n",
    "        overlap=2000, \n",
    "        signal_length_seconds=sig_len,\n",
    "        min_samples_each_word=3000\n",
    "        )\n",
    "    \n",
    "\n",
    "    sr = 16000\n",
    "    D, wp, ma, mb = dtw_distance(generated, actual, sr=sr, hop_length=64)\n",
    "    delim_fa, delim_fb = calculate_delimiter_timing_mapping(delimiters, wp, hop_length=64)\n",
    "    # delim_fb = direct_convert(delim_fa, delim_fb)\n",
    "    # delim_fb_mod = np.array([0,-0.05,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "    # delim_fb = delim_fb + delim_fb_mod\n",
    "    print(f\"delim_fa: {delim_fa}\")\n",
    "    print(f\"delim_fb: {delim_fb}\")\n",
    "    plot_melspecs_with_lines(ma, mb, delim_fa, delim_fb, hop_length=64, labels=sentence_list+[\"end\"])\n",
    "\n",
    "    broken_actual = breakupAudio(actual, delim_fb, silence_duration=2,feathering = 0)\n",
    "    broken_generated = breakupAudio(generated, delim_fa, silence_duration=2,feathering = 0)\n",
    "\n",
    "    return (\n",
    "        Audio(data=generated, rate=16000), Audio(data=actual, rate=16000),\n",
    "         Audio(data=broken_generated, rate=16000),Audio(data=broken_actual, rate=16000)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "a,b,c,d = try_dtw()\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import sklearn.model_selection\n",
    "from hparams import Hparams_michigan\n",
    "from dataset import read_michigan_dataset_index\n",
    "\n",
    "def get_data_loader_michigan(args):\n",
    "        # dataset_root=args['dataset_root'],\n",
    "        # split=split,\n",
    "        # sampling_rate=args['sampling_rate'],\n",
    "        # sample_length=args['sample_length'],\n",
    "        # frame_size=args['frame_size'],\n",
    "    index = read_michigan_dataset_index()\n",
    "    tone_classes = index[\"toneclass\"].values\n",
    "    ids = list(range(len(index)))\n",
    "    train_ids, test_ids= sklearn.model_selection.train_test_split(ids, test_size=0.2, random_state=42, shuffle=True, stratify=tone_classes)\n",
    "\n",
    "    train_index = index.iloc[train_ids]\n",
    "    test_index = index.iloc[test_ids]\n",
    "\n",
    "    train_ds = dataset.DatasetMichigan(\n",
    "        dataset_index=train_index, \n",
    "        dataset_root=args['dataset_root'], \n",
    "        sampling_rate=args['sampling_rate'], \n",
    "        preload_audio=args['preload_audio'],\n",
    "        sample_length=args['sample_length'],\n",
    "        pad_audio=args['pad_audio'],\n",
    "        )\n",
    "    \n",
    "    test_ds = dataset.DatasetMichigan(\n",
    "        dataset_index=test_index, \n",
    "        dataset_root=args['dataset_root'], \n",
    "        sampling_rate=args['sampling_rate'], \n",
    "        preload_audio=args['preload_audio'],\n",
    "        sample_length=args['sample_length'],\n",
    "        pad_audio=args['pad_audio'],\n",
    "        )\n",
    "    return train_ds, test_ds\n",
    "get_data_loader_michigan(Hparams_michigan.args)[0].plot_item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_loader_michigan(Hparams_michigan.args)[1].plot_item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval.sonify\n",
    "from IPython.display import Audio\n",
    "sr = 22050\n",
    "\n",
    "y_sweep = librosa.chirp(fmin=librosa.note_to_hz('C3'),\n",
    "                        fmax=librosa.note_to_hz('C5'),\n",
    "                        sr=sr,\n",
    "                        duration=1)\n",
    "\n",
    "Audio(data=y_sweep, rate=sr)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
