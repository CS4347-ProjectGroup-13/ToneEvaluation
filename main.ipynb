{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27470f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from hparams import *\n",
    "from dataset import get_data_loader, get_data_loader_michigan, get_data_loader_synthesized_michigan\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa9f7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:09<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256, 5])\n",
      "torch.Size([8, 256])\n",
      "torch.Size([8, 256])\n",
      "Congrats!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#new jupyter notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "for data in tqdm(train_loader):\n",
    "    mel_spectrogram, yin, pyin = data#move_data_to_device(data, 'cpu')\n",
    "    print(mel_spectrogram.shape)\n",
    "    print(yin.shape)\n",
    "    print(pyin.shape)\n",
    "    #assert list(x.shape) == [8, 250, 256]  # shape in [B, T, D],\n",
    "                                # i.e., [Batch size, num of frame per sample, spectrogram feature dimension]\n",
    "    #assert list(onset.shape) == list(offset.shape) == list(octave.shape) == list(pitch_class.shape) == [8, 250]\n",
    "    break\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b11fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the Michigan dataset\n",
    "train_ds, test_ds, data_loader_train, data_loader_test = get_data_loader_michigan(args=Hparams_michigan.args, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d28c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/246 [01:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Batch, feature)\n",
      "Spectrogram: torch.Size([32, 128, 75]) <class 'torch.Tensor'>\n",
      "Yin: torch.Size([32, 481]) <class 'torch.Tensor'>\n",
      "Pyin: torch.Size([32, 501]) <class 'torch.Tensor'>\n",
      "Word: 32 <class 'tuple'>\n",
      "Toneclass: torch.Size([32]) <class 'torch.Tensor'> tensor([1, 1, 4, 2, 4, 1, 2, 3, 1, 2, 2, 4, 3, 2, 3, 4, 3, 1, 4, 1, 2, 1, 4, 2,\n",
      "        3, 1, 4, 1, 1, 4, 3, 1])\n",
      "Congrats!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# to plot\n",
    "# train_ds.plot_item(0)\n",
    "\n",
    "# data loading\n",
    "for data in tqdm(data_loader_train):\n",
    "    mel_spectrogram_normalised_log_scale_torch, yin_normalised_torch, pyin_normalised_torch, word, toneclass = data\n",
    "    print(f\"(Batch, feature)\")\n",
    "    print(f\"Spectrogram: {mel_spectrogram_normalised_log_scale_torch.shape} {type(mel_spectrogram_normalised_log_scale_torch)}\")\n",
    "    print(f\"Yin: {yin_normalised_torch.shape} {type(yin_normalised_torch)}\")\n",
    "    print(f\"Pyin: {pyin_normalised_torch.shape} {type(pyin_normalised_torch)}\")\n",
    "    print(f\"Word: {len(word)} {type(word)}\")\n",
    "    print(f\"Toneclass: {toneclass.shape} {type(toneclass)} {toneclass}\")\n",
    "    break\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.buffer = {}\n",
    "\n",
    "    def update(self, out, tgt, loss):\n",
    "        with torch.no_grad():\n",
    "            out = out.argmax(dim=1)\n",
    "            out = torch.flatten(out)\n",
    "            tgt = torch.flatten(tgt)\n",
    "\n",
    "            acc = accuracy_score(tgt.cpu(), out.cpu())\n",
    "\n",
    "            batch_metric = {\n",
    "                'loss': loss.item(),\n",
    "                'accuracy': acc,\n",
    "            }\n",
    "\n",
    "            for k in batch_metric:\n",
    "                if k in self.buffer:\n",
    "                    self.buffer[k].append(batch_metric[k])\n",
    "                else:\n",
    "                    self.buffer[k] = [batch_metric[k]]\n",
    "\n",
    "    def get_value(self):\n",
    "        for k in self.buffer:\n",
    "            self.buffer[k] = sum(self.buffer[k]) / len(self.buffer[k])\n",
    "        ret = self.buffer\n",
    "        self.buffer = {}\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0992609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "\n",
    "\n",
    "def fit(model, args, learning_params):\n",
    "    # Set paths\n",
    "    save_model_dir = f\"{args['save_model_dir']}{model.feat_dim}_lr-{learning_params['lr']}\"\n",
    "    if not os.path.exists(save_model_dir):\n",
    "        os.mkdir(save_model_dir)\n",
    "\n",
    "    model.to(args['device'])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_params['lr'])\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    metric = Metrics()\n",
    "\n",
    "    # Start training\n",
    "    print('Start training...')\n",
    "    start_time = time.time()\n",
    "    best_model_id = -1\n",
    "    min_valid_loss = 10000\n",
    "    prev_loss = 10000\n",
    "    threshold = 1e-6\n",
    "\n",
    "    for epoch in range(1, learning_params['epoch'] + 1):\n",
    "        model.train()\n",
    "        \n",
    "        # Train\n",
    "        pbar = tqdm(data_loader_train)\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            mel_spectrogram_normalised_log_scale_torch, yin_normalised_torch, pyin_normalised_torch, word, tone_class = batch\n",
    "            tone_class -= 1 # 0-index\n",
    "        \n",
    "            x = mel_spectrogram_normalised_log_scale_torch.to(args['device'])\n",
    "            x = x[:, None, :, :]\n",
    "            tgt = tone_class.to(args['device'])\n",
    "            out = model(x)\n",
    "            loss = loss_func(out, tgt)\n",
    "            metric.update(out, tgt, loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_description('Epoch {}, Loss: {:.4f}'.format(epoch, loss.item()))\n",
    "        metric_train = metric.get_value()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(data_loader_test):\n",
    "                mel_spectrogram_normalised_log_scale_torch, yin_normalised_torch, pyin_normalised_torch, word, tone_class = batch\n",
    "                tone_class -= 1 # 0-index\n",
    "\n",
    "                x = mel_spectrogram_normalised_log_scale_torch.to(args['device'])\n",
    "                x = x[:, None, :, :]\n",
    "                tgt = tone_class.to(args['device'])\n",
    "                out = model(x)\n",
    "                loss = loss_func(out, tgt)\n",
    "                metric.update(out, tgt, loss)\n",
    "        metric_test = metric.get_value()\n",
    "\n",
    "        # Logging\n",
    "        print('[Epoch {:02d}], Train Loss: {:.5f}, Valid Loss {:.5f}, Time {:.2f}s'.format(\n",
    "            epoch, metric_train['loss'], metric_test['loss'], time.time() - start_time,\n",
    "        ))\n",
    "        print('Split Train Loss, Accuracy: Loss {:.4f} | Accuracy {:.4f}'.format(\n",
    "            metric_train['loss'],\n",
    "            metric_train['accuracy']\n",
    "        ))\n",
    "        print('Split Test Loss, Accuracy: Loss {:.4f} | Accuracy {:.4f}'.format(\n",
    "            metric_test['loss'],\n",
    "            metric_test['accuracy']\n",
    "        ))\n",
    "\n",
    "        # Save the best model\n",
    "        if metric_test['loss'] < min_valid_loss:\n",
    "            min_valid_loss = metric_test['loss']\n",
    "            best_model_id = epoch\n",
    "\n",
    "            save_dict = model.state_dict()\n",
    "            target_model_path = save_model_dir + '/best_model.pth'\n",
    "            torch.save(save_dict, target_model_path)\n",
    "\n",
    "        if abs(metric_test['loss'] - prev_loss) < threshold:\n",
    "            break\n",
    "\n",
    "        prev_loss = metric_test['loss']\n",
    "\n",
    "    print('Training done in {:.1f} minutes.'.format((time.time() - start_time) / 60))\n",
    "    return best_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7899e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/246 [02:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\junji\\Git\\ToneEvaluation\\main.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m learning_params \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m10\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-3\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m }\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m ToneEval_Base(input_shape\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m75\u001b[39m), feat_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m fit(model, args\u001b[39m=\u001b[39;49mHparams\u001b[39m.\u001b[39;49margs, learning_params\u001b[39m=\u001b[39;49mlearning_params)\n",
      "\u001b[1;32mc:\\Users\\junji\\Git\\ToneEvaluation\\main.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(data_loader_train)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     mel_spectrogram_normalised_log_scale_torch, yin_normalised_torch, pyin_normalised_torch, word, tone_class \u001b[39m=\u001b[39m batch\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/junji/Git/ToneEvaluation/main.ipynb#X10sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     tone_class \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# 0-index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\junji\\anaconda3\\envs\\CS4347\\lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\junji\\anaconda3\\envs\\CS4347\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\junji\\anaconda3\\envs\\CS4347\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\junji\\anaconda3\\envs\\CS4347\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m   1284\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m-> 1285\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1286\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1287\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\junji\\anaconda3\\envs\\CS4347\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1121\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1134\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1135\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1136\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\junji\\anaconda3\\envs\\CS4347\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[0;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\junji\\anaconda3\\envs\\CS4347\\lib\\threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[0;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set learning params\n",
    "learning_params = {\n",
    "    'epoch': 10,\n",
    "    'lr': 1e-3,\n",
    "}\n",
    "\n",
    "model = ToneEval_Base(input_shape=(1, 128, 75), feat_dim=64)\n",
    "fit(model, args=Hparams.args, learning_params=learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b114d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7652: 100%|██████████| 246/246 [1:25:19<00:00, 20.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01], Train Loss: 0.78274, Valid Loss 0.76786, Time 6558.57s\n",
      "Split Train Loss, Accuracy: Loss 0.7827 | Accuracy 0.9630\n",
      "Split Test Loss, Accuracy: Loss 0.7679 | Accuracy 0.9768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.7550: 100%|██████████| 246/246 [1:24:37<00:00, 20.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02], Train Loss: 0.75677, Valid Loss 0.75173, Time 12944.62s\n",
      "Split Train Loss, Accuracy: Loss 0.7568 | Accuracy 0.9874\n",
      "Split Test Loss, Accuracy: Loss 0.7517 | Accuracy 0.9924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.7745: 100%|██████████| 246/246 [1:24:08<00:00, 20.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03], Train Loss: 0.75268, Valid Loss 0.74725, Time 19292.03s\n",
      "Split Train Loss, Accuracy: Loss 0.7527 | Accuracy 0.9912\n",
      "Split Test Loss, Accuracy: Loss 0.7473 | Accuracy 0.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.7446: 100%|██████████| 246/246 [1:25:38<00:00, 20.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04], Train Loss: 0.75633, Valid Loss 0.74972, Time 25742.89s\n",
      "Split Train Loss, Accuracy: Loss 0.7563 | Accuracy 0.9872\n",
      "Split Test Loss, Accuracy: Loss 0.7497 | Accuracy 0.9945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.7448: 100%|██████████| 246/246 [1:26:15<00:00, 21.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05], Train Loss: 0.75420, Valid Loss 0.75035, Time 32289.64s\n",
      "Split Train Loss, Accuracy: Loss 0.7542 | Accuracy 0.9898\n",
      "Split Test Loss, Accuracy: Loss 0.7503 | Accuracy 0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.7471: 100%|██████████| 246/246 [1:25:19<00:00, 20.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06], Train Loss: 0.75064, Valid Loss 0.74772, Time 38747.47s\n",
      "Split Train Loss, Accuracy: Loss 0.7506 | Accuracy 0.9931\n",
      "Split Test Loss, Accuracy: Loss 0.7477 | Accuracy 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.7727: 100%|██████████| 246/246 [1:24:57<00:00, 20.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 07], Train Loss: 0.74928, Valid Loss 0.74548, Time 45152.52s\n",
      "Split Train Loss, Accuracy: Loss 0.7493 | Accuracy 0.9947\n",
      "Split Test Loss, Accuracy: Loss 0.7455 | Accuracy 0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.7601: 100%|██████████| 246/246 [1:25:10<00:00, 20.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 08], Train Loss: 0.75471, Valid Loss 0.74520, Time 51597.63s\n",
      "Split Train Loss, Accuracy: Loss 0.7547 | Accuracy 0.9888\n",
      "Split Test Loss, Accuracy: Loss 0.7452 | Accuracy 0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.7437: 100%|██████████| 246/246 [1:24:49<00:00, 20.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 09], Train Loss: 0.74881, Valid Loss 0.74909, Time 57999.69s\n",
      "Split Train Loss, Accuracy: Loss 0.7488 | Accuracy 0.9947\n",
      "Split Test Loss, Accuracy: Loss 0.7491 | Accuracy 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.7729: 100%|██████████| 246/246 [1:25:08<00:00, 20.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10], Train Loss: 0.75100, Valid Loss 0.74900, Time 64439.55s\n",
      "Split Train Loss, Accuracy: Loss 0.7510 | Accuracy 0.9929\n",
      "Split Test Loss, Accuracy: Loss 0.7490 | Accuracy 0.9950\n",
      "Training done in 1074.0 minutes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set learning params\n",
    "learning_params = {\n",
    "    'epoch': 10,\n",
    "    'lr': 1e-3,\n",
    "}\n",
    "\n",
    "model = ToneEval_Base(input_shape=(1, 128, 75), feat_dim=512)\n",
    "fit(model, args=Hparams.args, learning_params=learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947fe830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7451: 100%|██████████| 246/246 [1:27:53<00:00, 21.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01], Train Loss: 0.78702, Valid Loss 0.75133, Time 6639.65s\n",
      "Split Train Loss, Accuracy: Loss 0.7870 | Accuracy 0.9564\n",
      "Split Test Loss, Accuracy: Loss 0.7513 | Accuracy 0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.7464: 100%|██████████| 246/246 [1:25:02<00:00, 20.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02], Train Loss: 0.75874, Valid Loss 0.77568, Time 13062.78s\n",
      "Split Train Loss, Accuracy: Loss 0.7587 | Accuracy 0.9848\n",
      "Split Test Loss, Accuracy: Loss 0.7757 | Accuracy 0.9682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.7442: 100%|██████████| 246/246 [1:26:08<00:00, 21.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03], Train Loss: 0.75964, Valid Loss 0.74767, Time 19565.92s\n",
      "Split Train Loss, Accuracy: Loss 0.7596 | Accuracy 0.9841\n",
      "Split Test Loss, Accuracy: Loss 0.7477 | Accuracy 0.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.7515: 100%|██████████| 246/246 [1:25:55<00:00, 20.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04], Train Loss: 0.74946, Valid Loss 0.75915, Time 26072.20s\n",
      "Split Train Loss, Accuracy: Loss 0.7495 | Accuracy 0.9944\n",
      "Split Test Loss, Accuracy: Loss 0.7592 | Accuracy 0.9839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.7437: 100%|██████████| 246/246 [1:27:35<00:00, 21.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05], Train Loss: 0.75443, Valid Loss 0.74918, Time 32672.48s\n",
      "Split Train Loss, Accuracy: Loss 0.7544 | Accuracy 0.9895\n",
      "Split Test Loss, Accuracy: Loss 0.7492 | Accuracy 0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.8037: 100%|██████████| 246/246 [1:26:37<00:00, 21.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06], Train Loss: 0.75328, Valid Loss 0.75276, Time 39219.67s\n",
      "Split Train Loss, Accuracy: Loss 0.7533 | Accuracy 0.9905\n",
      "Split Test Loss, Accuracy: Loss 0.7528 | Accuracy 0.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.7446: 100%|██████████| 246/246 [1:25:52<00:00, 20.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 07], Train Loss: 0.75212, Valid Loss 0.75630, Time 45734.89s\n",
      "Split Train Loss, Accuracy: Loss 0.7521 | Accuracy 0.9912\n",
      "Split Test Loss, Accuracy: Loss 0.7563 | Accuracy 0.9884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.8369: 100%|██████████| 246/246 [1:27:50<00:00, 21.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 08], Train Loss: 0.75342, Valid Loss 0.74403, Time 52365.54s\n",
      "Split Train Loss, Accuracy: Loss 0.7534 | Accuracy 0.9901\n",
      "Split Test Loss, Accuracy: Loss 0.7440 | Accuracy 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.7438: 100%|██████████| 246/246 [1:28:16<00:00, 21.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 09], Train Loss: 0.75207, Valid Loss 0.75403, Time 59019.99s\n",
      "Split Train Loss, Accuracy: Loss 0.7521 | Accuracy 0.9917\n",
      "Split Test Loss, Accuracy: Loss 0.7540 | Accuracy 0.9904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.7437: 100%|██████████| 246/246 [1:29:48<00:00, 21.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10], Train Loss: 0.75009, Valid Loss 0.74684, Time 65815.95s\n",
      "Split Train Loss, Accuracy: Loss 0.7501 | Accuracy 0.9934\n",
      "Split Test Loss, Accuracy: Loss 0.7468 | Accuracy 0.9970\n",
      "Training done in 1096.9 minutes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set learning params\n",
    "learning_params = {\n",
    "    'epoch': 10,\n",
    "    'lr': 1e-3,\n",
    "}\n",
    "\n",
    "model = ToneEval_Base(input_shape=(1, 128, 75))\n",
    "fit(model, args=Hparams.args, learning_params=learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fit_segmentation(model, train_data_loader, test_data_loader, args, learning_params):\n",
    "    # Set paths\n",
    "    save_model_dir = f\"{args['save_model_dir']}{model.feat_dim}_lr-{learning_params['lr']}\"\n",
    "    if not os.path.exists(save_model_dir):\n",
    "        os.mkdir(save_model_dir)\n",
    "\n",
    "    model.to(args['device'])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_params['lr'])\n",
    "    loss_func = SegmentLossFunc()\n",
    "    metric = SegmentMetrics(loss_func)\n",
    "\n",
    "    # Start training\n",
    "    print('Start training...')\n",
    "    start_time = time.time()\n",
    "    best_model_id = -1\n",
    "    min_valid_loss = 10000\n",
    "    prev_loss = 10000\n",
    "    threshold = 1e-6\n",
    "\n",
    "    for epoch in range(1, learning_params['epoch'] + 1):\n",
    "        model.train()\n",
    "        \n",
    "        # Train\n",
    "        pbar = tqdm(train_data_loader)\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            mel_spectrogram_normalised_log_scale_torch, onset_rolls, offset_rolls = batch\n",
    "        \n",
    "            x = mel_spectrogram_normalised_log_scale_torch.to(args['device'])\n",
    "            x = x[:, None, :, :]\n",
    "            tgt = (onset_rolls.to(args['device']), offset_rolls.to(args['device']))\n",
    "            out = model(x)\n",
    "            losses = loss_func.get_loss(out, tgt)\n",
    "            loss = losses[0]\n",
    "            metric.update(out, tgt, losses)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_description('Epoch {}, Loss: {:.4f}'.format(epoch, loss.item()))\n",
    "        metric_train = metric.get_value()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(test_data_loader):\n",
    "                mel_spectrogram_normalised_log_scale_torch, onset_rolls, offset_rolls = batch\n",
    "\n",
    "                x = mel_spectrogram_normalised_log_scale_torch.to(args['device'])\n",
    "                x = x[:, None, :, :]\n",
    "                tgt = (onset_rolls.to(args['device']), offset_rolls.to(args['device']))\n",
    "                out = model(x)\n",
    "                metric.update(out, tgt)\n",
    "        metric_test = metric.get_value()\n",
    "\n",
    "        # Logging\n",
    "        print('[Epoch {:02d}], Train Loss: {:.5f}, Valid Loss {:.5f}, Time {:.2f}s'.format(\n",
    "            epoch, metric_train['loss'], metric_test['loss'], time.time() - start_time,\n",
    "        ))\n",
    "        print('Split Train Loss, F1 Score: Onset Loss {:.4f} | Onset F1 Score {:.4f} | Offset Loss {:.4f} | Offset F1 Score {:.4f}'.format(\n",
    "            metric_train['onset_loss'],\n",
    "            metric_train['offset_loss'],\n",
    "            metric_train['onset_f1'],\n",
    "            metric_train['offset_f1']\n",
    "        ))\n",
    "        print('Split Test Loss, F1 Score: Onset Loss {:.4f} | Onset F1 Score {:.4f} | Offset Loss {:.4f} | Offset F1 Score {:.4f}'.format(\n",
    "            metric_test['onset_loss'],\n",
    "            metric_test['offset_loss'],\n",
    "            metric_test['onset_f1'],\n",
    "            metric_test['offset_f1']\n",
    "        ))\n",
    "\n",
    "        # Save the best model\n",
    "        if metric_test['loss'] < min_valid_loss:\n",
    "            min_valid_loss = metric_test['loss']\n",
    "            best_model_id = epoch\n",
    "\n",
    "            save_dict = model.state_dict()\n",
    "            target_model_path = save_model_dir + '/best_model.pth'\n",
    "            torch.save(save_dict, target_model_path)\n",
    "\n",
    "        if abs(metric_test['loss'] - prev_loss) < threshold:\n",
    "            break\n",
    "\n",
    "        prev_loss = metric_test['loss']\n",
    "\n",
    "    print('Training done in {:.1f} minutes.'.format((time.time() - start_time) / 60))\n",
    "    return best_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the synthesized Michigan dataset\n",
    "pipelineOptions = {}\n",
    "pipelineOptions['frame_per_sec'] = 50\n",
    "pipelineOptions['window_length'] = 2048\n",
    "pipelineOptions['n_fft'] = 2048\n",
    "pipelineOptions['n_mels'] = 256\n",
    "\n",
    "syn_train_ds, syn_test_ds, syn_data_loader_train, syn_data_loader_test = get_data_loader_synthesized_michigan(args=Hparams_synthesized_michigan.args, pipelineOptions=pipelineOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Batch, feature)\n",
      "Spectrogram: torch.Size([32, 256, 501]) <class 'torch.Tensor'>\n",
      "Onset roll: torch.Size([32, 501]) <class 'torch.Tensor'>\n",
      "Offset roll: torch.Size([32, 501]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:26<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# data loading\n",
    "for data in tqdm(syn_data_loader_train):\n",
    "    mel_spectrogram_normalised_log_scale_torch, onset_rolls, offset_rolls = data\n",
    "    print(f\"(Batch, feature)\")\n",
    "    print(f\"Spectrogram: {mel_spectrogram_normalised_log_scale_torch.shape} {type(mel_spectrogram_normalised_log_scale_torch)}\")\n",
    "    print(f\"Onset roll: {onset_rolls.shape} {type(onset_rolls)}\")\n",
    "    print(f\"Offset roll: {offset_rolls.shape} {type(offset_rolls)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1457: 100%|██████████| 25/25 [00:31<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01], Train Loss: 0.34599, Valid Loss 0.30307, Time 54.67s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.1703 | Onset F1 Score 0.1757 | Offset Loss 0.1519 | Offset F1 Score 0.0012\n",
      "Split Test Loss, F1 Score: Onset Loss 0.1575 | Onset F1 Score 0.1456 | Offset Loss 0.2422 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1593: 100%|██████████| 25/25 [00:29<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02], Train Loss: 0.15267, Valid Loss 0.19943, Time 107.11s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0710 | Onset F1 Score 0.0816 | Offset Loss 0.2335 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0793 | Onset F1 Score 0.1201 | Offset Loss 0.2395 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.1516: 100%|██████████| 25/25 [00:29<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03], Train Loss: 0.14871, Valid Loss 0.14202, Time 159.89s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0705 | Onset F1 Score 0.0782 | Offset Loss 0.2336 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0669 | Onset F1 Score 0.0752 | Offset Loss 0.2444 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.1379: 100%|██████████| 25/25 [00:28<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04], Train Loss: 0.14769, Valid Loss 0.14268, Time 210.52s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0700 | Onset F1 Score 0.0777 | Offset Loss 0.2334 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0675 | Onset F1 Score 0.0751 | Offset Loss 0.2413 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.1380: 100%|██████████| 25/25 [00:27<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05], Train Loss: 0.14710, Valid Loss 0.14498, Time 261.67s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0698 | Onset F1 Score 0.0773 | Offset Loss 0.2334 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0686 | Onset F1 Score 0.0763 | Offset Loss 0.2391 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.1483: 100%|██████████| 25/25 [00:28<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06], Train Loss: 0.14705, Valid Loss 0.14228, Time 313.43s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0699 | Onset F1 Score 0.0772 | Offset Loss 0.2334 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0670 | Onset F1 Score 0.0753 | Offset Loss 0.2429 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.1491: 100%|██████████| 25/25 [00:29<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 07], Train Loss: 0.14681, Valid Loss 0.14207, Time 366.77s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0698 | Onset F1 Score 0.0770 | Offset Loss 0.2335 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0673 | Onset F1 Score 0.0748 | Offset Loss 0.2420 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.1436: 100%|██████████| 25/25 [00:29<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 08], Train Loss: 0.14646, Valid Loss 0.14182, Time 420.39s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0697 | Onset F1 Score 0.0768 | Offset Loss 0.2335 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0671 | Onset F1 Score 0.0747 | Offset Loss 0.2428 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.1442: 100%|██████████| 25/25 [00:29<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 09], Train Loss: 0.14620, Valid Loss 0.14258, Time 473.60s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0696 | Onset F1 Score 0.0766 | Offset Loss 0.2334 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0676 | Onset F1 Score 0.0749 | Offset Loss 0.2412 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.1422: 100%|██████████| 25/25 [00:29<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10], Train Loss: 0.14674, Valid Loss 0.14411, Time 525.38s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0700 | Onset F1 Score 0.0767 | Offset Loss 0.2335 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0675 | Onset F1 Score 0.0766 | Offset Loss 0.2445 | Offset F1 Score 0.0000\n",
      "Training done in 8.8 minutes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set learning params\n",
    "learning_params = {\n",
    "    'epoch': 10,\n",
    "    'lr': 1e-3,\n",
    "}\n",
    "\n",
    "model = ToneSegment_Base(input_shape=(1, 256, 501))\n",
    "fit_segmentation(model, syn_data_loader_train, syn_data_loader_test, args=Hparams_synthesized_michigan.args, learning_params=learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the synthesized Michigan dataset\n",
    "args = Hparams_synthesized_michigan.args\n",
    "args['dataset_root'] =  os.path.join(os.getcwd(), 'data_synthesized_large')\n",
    "\n",
    "pipelineOptions = {}\n",
    "pipelineOptions['frame_per_sec'] = 50\n",
    "pipelineOptions['window_length'] = 2048\n",
    "pipelineOptions['n_fft'] = 2048\n",
    "pipelineOptions['n_mels'] = 256\n",
    "\n",
    "syn_train_ds, syn_test_ds, syn_data_loader_train, syn_data_loader_test = get_data_loader_synthesized_michigan(args=args, pipelineOptions=pipelineOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1501: 100%|██████████| 250/250 [02:00<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01], Train Loss: 0.16291, Valid Loss 0.14586, Time 165.96s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0783 | Onset F1 Score 0.0847 | Offset Loss 0.2296 | Offset F1 Score 0.0001\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0691 | Onset F1 Score 0.0768 | Offset Loss 0.2367 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1459: 100%|██████████| 250/250 [01:51<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02], Train Loss: 0.14500, Valid Loss 0.14825, Time 320.80s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0690 | Onset F1 Score 0.0760 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0704 | Onset F1 Score 0.0779 | Offset Loss 0.2366 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.1383: 100%|██████████| 250/250 [01:47<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03], Train Loss: 0.14452, Valid Loss 0.14505, Time 471.08s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0689 | Onset F1 Score 0.0756 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0689 | Onset F1 Score 0.0762 | Offset Loss 0.2367 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.1487: 100%|██████████| 250/250 [01:49<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04], Train Loss: 0.14421, Valid Loss 0.14642, Time 623.62s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0688 | Onset F1 Score 0.0754 | Offset Loss 0.2363 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0694 | Onset F1 Score 0.0770 | Offset Loss 0.2366 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.1579: 100%|██████████| 250/250 [01:50<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05], Train Loss: 0.14405, Valid Loss 0.15147, Time 778.84s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0688 | Onset F1 Score 0.0752 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0711 | Onset F1 Score 0.0803 | Offset Loss 0.2367 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.1414: 100%|██████████| 250/250 [01:51<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06], Train Loss: 0.14390, Valid Loss 0.14579, Time 935.33s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0688 | Onset F1 Score 0.0751 | Offset Loss 0.2363 | Offset F1 Score 0.0001\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0692 | Onset F1 Score 0.0766 | Offset Loss 0.2366 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.1402: 100%|██████████| 250/250 [01:51<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 07], Train Loss: 0.14375, Valid Loss 0.15936, Time 1091.54s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0687 | Onset F1 Score 0.0750 | Offset Loss 0.2364 | Offset F1 Score 0.0002\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0718 | Onset F1 Score 0.0875 | Offset Loss 0.2366 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.1473: 100%|██████████| 250/250 [01:51<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 08], Train Loss: 0.14372, Valid Loss 0.14442, Time 1249.40s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0688 | Onset F1 Score 0.0749 | Offset Loss 0.2364 | Offset F1 Score 0.0001\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0687 | Onset F1 Score 0.0757 | Offset Loss 0.2366 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.1379: 100%|██████████| 250/250 [01:55<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 09], Train Loss: 0.14354, Valid Loss 0.14757, Time 1413.34s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0687 | Onset F1 Score 0.0749 | Offset Loss 0.2364 | Offset F1 Score 0.0001\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0687 | Onset F1 Score 0.0789 | Offset Loss 0.2367 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.1457: 100%|██████████| 250/250 [01:55<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10], Train Loss: 0.14341, Valid Loss 0.14817, Time 1575.50s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0686 | Onset F1 Score 0.0748 | Offset Loss 0.2364 | Offset F1 Score 0.0002\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0716 | Onset F1 Score 0.0766 | Offset Loss 0.2365 | Offset F1 Score 0.0000\n",
      "Training done in 26.3 minutes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set learning params\n",
    "learning_params = {\n",
    "    'epoch': 10,\n",
    "    'lr': 1e-3,\n",
    "}\n",
    "\n",
    "model = ToneSegment_Base(input_shape=(1, 256, 501))\n",
    "fit_segmentation(model, syn_data_loader_train, syn_data_loader_test, args=args, learning_params=learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1467: 100%|██████████| 250/250 [01:56<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01], Train Loss: 0.15631, Valid Loss 0.14532, Time 158.31s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0750 | Onset F1 Score 0.0813 | Offset Loss 0.2216 | Offset F1 Score 0.0001\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0690 | Onset F1 Score 0.0763 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1523: 100%|██████████| 250/250 [01:44<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02], Train Loss: 0.14471, Valid Loss 0.14489, Time 304.19s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0689 | Onset F1 Score 0.0759 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0687 | Onset F1 Score 0.0762 | Offset Loss 0.2366 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.1380: 100%|██████████| 250/250 [01:43<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03], Train Loss: 0.14424, Valid Loss 0.15055, Time 447.71s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0689 | Onset F1 Score 0.0754 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0706 | Onset F1 Score 0.0800 | Offset Loss 0.2365 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.1496: 100%|██████████| 250/250 [01:45<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04], Train Loss: 0.14383, Valid Loss 0.14361, Time 593.21s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0688 | Onset F1 Score 0.0750 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0688 | Onset F1 Score 0.0748 | Offset Loss 0.2367 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.1418: 100%|██████████| 250/250 [01:47<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05], Train Loss: 0.14373, Valid Loss 0.15190, Time 740.97s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0688 | Onset F1 Score 0.0750 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0720 | Onset F1 Score 0.0799 | Offset Loss 0.2365 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.1432: 100%|██████████| 250/250 [01:45<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06], Train Loss: 0.14359, Valid Loss 0.14677, Time 886.29s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0688 | Onset F1 Score 0.0748 | Offset Loss 0.2363 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0704 | Onset F1 Score 0.0764 | Offset Loss 0.2370 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.1483: 100%|██████████| 250/250 [01:42<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 07], Train Loss: 0.14355, Valid Loss 0.15822, Time 1030.46s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0688 | Onset F1 Score 0.0748 | Offset Loss 0.2363 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0728 | Onset F1 Score 0.0854 | Offset Loss 0.2366 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.1366: 100%|██████████| 250/250 [01:44<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 08], Train Loss: 0.14336, Valid Loss 0.18494, Time 1176.83s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0687 | Onset F1 Score 0.0746 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0840 | Onset F1 Score 0.1010 | Offset Loss 0.2367 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.1439: 100%|██████████| 250/250 [01:44<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 09], Train Loss: 0.14315, Valid Loss 0.18331, Time 1325.60s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0687 | Onset F1 Score 0.0745 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0781 | Onset F1 Score 0.1052 | Offset Loss 0.2365 | Offset F1 Score 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.1420: 100%|██████████| 250/250 [01:51<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10], Train Loss: 0.14306, Valid Loss 0.19601, Time 1477.03s\n",
      "Split Train Loss, F1 Score: Onset Loss 0.0686 | Onset F1 Score 0.0744 | Offset Loss 0.2364 | Offset F1 Score 0.0000\n",
      "Split Test Loss, F1 Score: Onset Loss 0.0809 | Onset F1 Score 0.1151 | Offset Loss 0.2365 | Offset F1 Score 0.0000\n",
      "Training done in 24.6 minutes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set learning params\n",
    "learning_params = {\n",
    "    'epoch': 10,\n",
    "    'lr': 1e-3,\n",
    "}\n",
    "\n",
    "model = ToneSegment_Enhanced(input_shape=(1, 256, 501))\n",
    "fit_segmentation(model, syn_data_loader_train, syn_data_loader_test, args=args, learning_params=learning_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4347",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
